# 🚀 LLM Evaluation Playground
#live link - https://aievaluationsuite.netlify.app/
Welcome to the ultimate sandbox for poking, prodding, and properly judging your favorite Large Language Models — like GPT-4o, Claude, or any Hugging Face model you can throw at it.

This isn't just a prompt tester.
It's a full-blown **LLM Evaluation Dashboard** ⚙️

---

## 🎯 Why?

Because sometimes "It sounds smart" isn't enough.  
We need to know:

- ❓ Is it safe?
- ❓ Did it hallucinate?
- ❓ Can I trust this in production?
- ✅ And... who actually gave the best response?

---

## 🧠 What it does

- 🔁 **Compare LLMs side-by-side** in real-time
- 🧪 **Stress-test** models with prompt variations
- 💥 **Track hallucinations** and toxicity
- ✅ **A/B testing** across models and versions
- 📝 **Manual scoring** (readability, accuracy, usefulness)
- 💬 **Leave comments**, save feedback
- 📊 **See analytics & trends**
- 🧰 **Use prompt templates** when creativity runs dry

---

## 🛠 Tech Stack

| Layer         | Tech                                |
|---------------|-------------------------------------|
| Frontend      | React + TypeScript + Tailwind CSS   |
| LLM API       | Hugging Face hosted models          |
| Persistence   | LocalStorage (no backend!)          |
| Hosting       | Netlify                             |
| Dev Workflow  | CI/CD, version tracking, fake drift |
| UX touches    | Toasts, loading states, smooth UI   |

---

## 🧩 Features (and yes, they're actually fun)

- 🎭 Prompt Template Library  
- 🧠 Optimize Prompt (automated cleanup)  
- 🧪 A/B Testing Playground  
- 📈 Analytics Panel with Trend Tracking  
- 🧬 Versioning + Simulated Drift  
- 🚀 CI/CD for continuous deploys  

---


