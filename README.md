# ğŸš€ LLM Evaluation Playground
#live link - https://aievaluationsuite.netlify.app/
Welcome to the ultimate sandbox for poking, prodding, and properly judging your favorite Large Language Models â€” like GPT-4o, Claude, or any Hugging Face model you can throw at it.

This isn't just a prompt tester.
It's a full-blown **LLM Evaluation Dashboard** âš™ï¸

---

## ğŸ¯ Why?

Because sometimes "It sounds smart" isn't enough.  
We need to know:

- â“ Is it safe?
- â“ Did it hallucinate?
- â“ Can I trust this in production?
- âœ… And... who actually gave the best response?

---

## ğŸ§  What it does

- ğŸ” **Compare LLMs side-by-side** in real-time
- ğŸ§ª **Stress-test** models with prompt variations
- ğŸ’¥ **Track hallucinations** and toxicity
- âœ… **A/B testing** across models and versions
- ğŸ“ **Manual scoring** (readability, accuracy, usefulness)
- ğŸ’¬ **Leave comments**, save feedback
- ğŸ“Š **See analytics & trends**
- ğŸ§° **Use prompt templates** when creativity runs dry

---

## ğŸ›  Tech Stack

| Layer         | Tech                                |
|---------------|-------------------------------------|
| Frontend      | React + TypeScript + Tailwind CSS   |
| LLM API       | Hugging Face hosted models          |
| Persistence   | LocalStorage (no backend!)          |
| Hosting       | Netlify                             |
| Dev Workflow  | CI/CD, version tracking, fake drift |
| UX touches    | Toasts, loading states, smooth UI   |

---

## ğŸ§© Features (and yes, they're actually fun)

- ğŸ­ Prompt Template Library  
- ğŸ§  Optimize Prompt (automated cleanup)  
- ğŸ§ª A/B Testing Playground  
- ğŸ“ˆ Analytics Panel with Trend Tracking  
- ğŸ§¬ Versioning + Simulated Drift  
- ğŸš€ CI/CD for continuous deploys  

---


